<!--
`ivx-qtype-usermedia` can be used to record audio and video from the devices microphone and camera. It uses only the
standard APIs **Web Audio API** and **UserMedia API**. It also overlays the live video output with a audio visualization.

Please note that this component is a fancy wrapper around those APIs and provides you with the essential functionality
for recording video and audio. There are also some filters added, which the user can apply on the video. These filters
are merged into the video stream. They're only applied via the CSS3 selector `filter`.
The APIs are currently (January 2015) only available in the Blink engine. Trident and Gecko doesn't provide those API
and because there's no fallback mechanism implemented in this component, you should implement those by yourself or
use another implementation to fix this.

Please note too that the audio and video stream can not be recorded as one blob record. This is because of the API
limitation. As a result of that a video exists of a recorded video stream and a recorded audio stream. The recorded
audio and video will be started at the same time, when the user plays a recorded video. There could be some asynchronity
by playing the video

Use the attribute `mode` to switch between the types of recording (video (audio+video), audio (only audio), image).

== **Please Note** ==

This component only work on desktop browsers reliable:

- iOS 8+ on Safari only supports the Audio WebAPI, so there's no Video/Image access available
- Android 4.4.4+ on Chrome 40+ supports Audio and Video, 5+ integrates the evergreen Chrome, so it works there too
- Windows Internet Explorer does NOT support any of the used APIs

== **Please Note** ==

You can use the `ivx-qtype-usermedia` element like the following.

Example:

    <ivx-qtype-usermedia mode="video"></ivx-qtype-usermedia>

@group Inovex Survey Elements
@element ivx-qtype-usermedia
@homepage inovex.de
-->

<link rel="import" href="../../bower_components/polymer/polymer.html">
<link rel="import" href="../../bower_components/core-icons/av-icons.html">
<link rel="import" href="../../bower_components/core-icons/image-icons.html">

<link rel="import" href="../ivx-core-audio/ivx-core-audio.html">
<link rel="import" href="../ivx-core-image/ivx-core-image.html">
<link rel="import" href="../ivx-core-video/ivx-core-video.html">
<link rel="import" href="../ivx-core-usermedia/ivx-core-usermedia.html">
<link rel="import" href="../ivx-core-mediastreamtrack/ivx-core-mediastreamtrack.html">

<link rel="import" href="../mixins-helper.html">

<polymer-element name="ivx-qtype-usermedia" attributes="mode visualizeAudio">
  <template>
    <!-- Stylesheet -->
    <link rel="stylesheet" href="ivx-qtype-usermedia.css">

    <div vertical layout center>

      <!-- START video / device container -->
      <div horizontal layout center>

        <!-- START video container -->
        <div id="videoContainer" class="[[ mode ]]" flex>

          <!-- START UI State: ACCESS GRANTED -->
          <template if="{{ _currentUIState == 'deviceAccessGranted' }}">
            <template if="[[ mode == 'video' || mode == 'image' ]]">
              <!-- live camera -->
              <video flex id="liveVideo" class="{{ _filterSelected }}" muted></video>
            </template>

            <!-- audio visualizer overlay -->
            <template if="[[ mode == 'audio' || mode == 'video' ]]">
              <ivx-core-audio
                id="audioComponent"
                visualizeAudio="{{ visualizeAudio }}"
                gain="{{ _gainValue }}"
                >
              </ivx-core-audio>
            </template>
          </template>
          <!-- END UI State: ACCESS GRANTED -->

          <!-- START UI State: MEDIA RECORDED -->
          <template if="{{ _currentUIState == 'mediaRecorded' }}">
            <!-- playback from existing video -->
            <template if="[[ mode == 'video' ]]">
              <video id="playbackVideo"
                     class="{{ _filterSelected }}" on-ended="{{ onPlaybackEnded }}"
                     on-timeupdate="{{ onPlaybackTimeupdate }}"></video>
              <audio id="playbackAudio" src="{{ _audioObjectURL }}" volume="1.0" type="audio/wave"></audio>
              <div id="progressLine"></div>
            </template>

            <template if="[[ mode == 'audio' ]]">
              <audio id="playbackAudio" src="{{ _audioObjectURL }}" volume="1.0" type="audio/wave"
                     on-ended="{{ onPlaybackEnded }}" on-timeupdate="{{ onPlaybackTimeupdate }}"></audio>
              <div id="progressLine"></div>
            </template>

            <template if="[[ mode == 'image' ]]">
              <!-- picture from camera -->
              <img src="{{ _imageDataURL }}" class="{{ _filterSelected }}">
            </template>
          </template>
          <!-- END UI State: MEDIA RECORDED -->
        </div>
        <!-- END video container -->

        <!-- START device list -->
        <div self-end> <!-- positioning on bottom -->
          <div id="deviceList" layout vertical>
            <template if="{{ (mode == 'video' || mode == 'image') && _cameras.length > 0 }}">
              <paper-radio-group selected="{{ _selectedCameraId }}" class="spaceToTop" layout vertical>
                <template repeat="{{ device, index in _cameras }}">
                  <paper-radio-button
                    name="{{ device.id }}"
                    label="Kamera {{ index + 1 }} {{ device.facing != '' ? '(' + device.facing + ')' : '' }}"
                    on-tap="{{ onCameraTapped }}">
                  </paper-radio-button>
                </template>
              </paper-radio-group>
            </template>

            <template if="[[ mode == 'video' || mode == 'audio' ]]">
              <paper-radio-group selected="{{ _selectedMicrophoneId }}" class="spaceToTop" layout vertical>
                <template repeat="{{ device, index in _microphones }}">
                  <paper-radio-button
                    name="{{ device.id }}"
                    label="Mikrofon {{ index + 1 }} {{ device.facing != '' ? '(' + device.facing + ')' : '' }}"
                    on-tap="{{ onMicrophoneTapped }}">
                  </paper-radio-button>
                </template>
              </paper-radio-group>

              <paper-slider min="0" max="100" value="[[ _gainValueInit * 100 ]]"
                            on-immediate-value-change="{{ onMicrophoneVolumeChange }}"></paper-slider>
            </template>
          </div>
        </div>
        <!-- END device list -->
      </div>
      <!-- END video / device container -->

      <!-- START button container -->
      <div id="buttonContainer" class="spaceToTop">
        <!-- First enable camera/mic to record anything -->
        <template if="{{ _currentUIState == 'waitingForDeviceAccess' }}">
          <!-- activate button -->
          <paper-button raised on-tap="{{ onEnableTapped }}" class="spaceToTop">
            <core-icon icon="av:videocam"></core-icon>
            <div>Zugriff aktivieren</div>
          </paper-button>
        </template>

        <template if="{{ _currentUIState == 'deviceAccessGranted' }}">
          <div center layout horizontal>
            <div center layout vertical>
              <template bind ref="audioVideoImageButtons"></template>
              <!-- filters -->
              <template if="[[ mode == 'video' || mode == 'image' ]]">
                <paper-radio-group selected="{{ _filterSelected }}" class="spaceToTop">
                  <template repeat="{{ filter, index in _filters }}">
                    <paper-radio-button
                      name="{{ filter }}"
                      label="{{ filter }}"
                      on-tap="{{ onFilterTapped }}">
                    </paper-radio-button>
                  </template>
                </paper-radio-group>
              </template>
            </div>
          </div>
        </template>

        <template if="{{ _currentUIState == 'mediaRecorded' }}">
          <div center layout horizontal>

            <template if="[[ mode == 'audio' || mode == 'video' ]]">
              <!-- play button -->
              <template if="{{ _currentPlayerState == 'stopped' || _currentPlayerState == 'paused' }}">
                <paper-button id="playButton" raised on-tap="{{ onPlayTapped }}">
                  <core-icon icon="av:play-arrow"></core-icon>
                </paper-button>
              </template>

              <!-- pause button -->
              <template if="{{ _currentPlayerState == 'playing' }}">
                <paper-button id="pauseButton" raised on-tap="{{ onPauseTapped }}">
                  <core-icon icon="av:pause"></core-icon>
                </paper-button>
              </template>

              <!-- stop button -->
              <paper-button id="stopButton" raised on-tap="{{ onStopTapped }}">
                <core-icon icon="av:stop"></core-icon>
              </paper-button>
            </template>

            <template if="[[ mode == 'audio' || mode == 'video' || mode == 'image' ]]">
              <!-- redo button -->
              <paper-button raised on-tap="{{ onRedoTapped }}">
                <core-icon icon="av:replay"></core-icon>
                <div>Aufnahme wiederholen</div>
              </paper-button>
            </template>
          </div>
        </template>
      </div>
      <!-- END button container -->

      <!-- START console container -->
      <div id="consoleContainer" class="spaceToTop">
        <template if="{{ (mode == 'audio' || mode == 'video') && _recordingNow }}">
          Recording ... {{ _recordingTime }}s
        </template>

        <template if="{{ mode == 'audio' && !_recordingNow && _currentUIState == 'mediaRecorded' }}">
          Recorded {{ _totalDuration }}s
        </template>

        <template if="{{ mode == 'video' && !_recordingNow && _currentUIState == 'mediaRecorded' }}">
          Frames captured: {{ _videoFramesCaptured }}, {{ _totalDuration }}s Video (~{{ _videoFps }} fps)
        </template>
      </div>
      <!-- END console container -->

    </div>

    <!-- START bound templates -->
    <template id="audioVideoImageButtons">
      <template if="[[ mode == 'audio' || mode == 'video' ]]">
        <paper-button id="recordButton" raised on-tap="{{ onRecordingTapped }}">

          <!-- glow/pulsate the record icon in recording mode -->
          <template if="{{ !_recordingNow }}">
            <core-icon icon="radio-button-off"></core-icon>
          </template>

          <template if="{{ _recordingNow }}">
            <core-icon icon="radio-button-on" class="pulsate"></core-icon>
          </template>

          <div>
            <template if="{{ !_recordingNow }}">Aufnahme starten</template>
            <template if="{{ _recordingNow }}">Aufnahme stoppen</template>
          </div>
        </paper-button>
      </template>

      <template if="[[ mode == 'image' ]]">
        <paper-button id="snapButton" raised on-tap="{{ onSnapTapped }}">
          <core-icon icon="image:photo-camera"></core-icon>
          <div>Bild aufnehmen</div>
        </paper-button>
      </template>
    </template>
    <!-- END bound templates -->

  </template>

  <script>
    (function () {
      Polymer(Polymer.mixin({
        /**
         * The usermedia mode to record from. Valid modes are `video`, `audio` and `image`.
         *
         * @attribute mode
         * @type String
         * @default 'video'
         */

        /**
         * Enables or disables the audio visualizer.
         *
         * @attribute visualizeAudio
         * @type Boolean
         * @default true
         */
        publish: {
          mode: 'video',
          visualizeAudio: true
        },

        // filters to apply on the video via CSS3
        _filters: ['Kein Filter', 'grayscale', 'sepia', 'invert', 'brightness', 'blur'],
        _filterSelected: 'Kein Filter',

        // the UI states
        _uiStateEnum: {
          WAITING_FOR_DEVICE_ACCESS: 'waitingForDeviceAccess',
          DEVICE_ACCESS_GRANTED: 'deviceAccessGranted',
          MEDIA_RECORDED: 'mediaRecorded'
        },
        _currentUIState: '',

        _playerStateEnum: {
          STOPPED: 'stopped',
          PLAYING: 'playing',
          PAUSED: 'paused'
        },
        //_currentPlayerState: '',
        _currentPlayerState: 'mediaStopped',

        // getUserMedia stream for all modes
        _stream: null,
        _microphones: [],
        _cameras: [],
        _selectedCameraId: '',
        _selectedMicrophoneId: '',

        // image specific
        _imageDataURL: null,

        // video specific
        _videoFramesCaptured: 0,
        _videoFps: 0,
        _videoBlob: null,
        _videoDim: {
          width: 320,
          height: 240
        },

        // audio specific
        _audioBlob: null,
        _gainValueInit: 0.8,
        _gainValue: 0.8,

        _recordingNow: false,
        _totalDuration: null,
        _recordingTime: 0,

        // components
        _audioComponent: null,
        _imageComponent: null,
        _videoComponent: null,

        ready: function () {
          this._currentUIState = this._uiStateEnum.WAITING_FOR_DEVICE_ACCESS;
        },

        // ===== UI triggered methods =====

        // all: event handler to initialize the UserMedia API
        onEnableTapped: function (event, detail, sender) {
          this._initGetUserMedia();
        },

        // all: event handler to dismiss the current recording to start a new one
        onRedoTapped: function (event, detail, sender) {
          this._initVideoAudio();
        },

        // audio: event handler to reinit the UserMedia stream, when the microphone source changes
        onMicrophoneTapped: function (event, detail, sender) {
          if (this._currentUIState == this._uiStateEnum.DEVICE_ACCESS_GRANTED && !this._recordingNow) {
            this._initGetUserMedia();
          }
        },

        // audio
        onMicrophoneVolumeChange: function (event, detail, sender) {
          if (this._currentUIState == this._uiStateEnum.DEVICE_ACCESS_GRANTED) {
            this._gainValue = sender.immediateValue / 100;
          }
        },

        // image: event handler to grap a picture of the current stream frame
        onSnapTapped: function (event, detail, sender) {
          this._takePicture();
          this._currentUIState = this._uiStateEnum.MEDIA_RECORDED;
        },

        // image, video: event handler to reinit the UserMedia stream, when the camera source changes
        onCameraTapped: function (event, detail, sender) {
          if (this._currentUIState == this._uiStateEnum.DEVICE_ACCESS_GRANTED && !this._recordingNow) {
            this._initGetUserMedia();
          }
        },

        // audio, video: event handler to start and stop recording
        onRecordingTapped: function (event, detail, sender) {
          this._recordingNow = !this._recordingNow;

          // recording
          if (this._recordingNow) {
            this._startRecord();
          } else {
            this._stopRecord();
          }
        },

        // audio, video: event handler to sync the current playback time with the visual progress bar
        onPlaybackTimeupdate: function (event, detail, sender) {
          var percentPlayed = (sender.currentTime / this._totalDuration) * 100;
          this._id('#progressLine').style.width = percentPlayed + '%';
        },

        // audio, video: event handler to start playing the recorded stream(s) (based on the current mode)
        onPlayTapped: function (event, detail, sender) {
          this._currentPlayerState = this._playerStateEnum.PLAYING;

          if (this.mode == 'video') {
            this._id('#playbackVideo').play();
          }
          this._id('#playbackAudio').play();
        },

        // audio, video: event handler to pause the playing audio/video (based on the current mode)
        onPauseTapped: function (event, detail, sender) {
          this._currentPlayerState = this._playerStateEnum.PAUSED;

          if (this.mode == 'video') {
            this._id('#playbackVideo').pause();
          }
          this._id('#playbackAudio').pause();
        },

        // audio, video: event handler to stop the playing audio/video (based on the current mode)
        onStopTapped: function (event, detail, sender) {
          this._currentPlayerState = this._playerStateEnum.STOPPED;

          if (this.mode == 'video') {
            this._id('#playbackVideo').load();
          }
          this._id('#playbackAudio').load();
        },

        // image, video: event handler to apply the selected filter
        onFilterTapped: function (event, detail, sender) {
          this._filterSelected = sender.getAttribute('name');
        },

        // video
        onPlaybackEnded: function (event, detail, sender) {
          this._currentPlayerState = this._playerStateEnum.STOPPED;
          this._id('#progressLine').style.width = '100%';
        },

        // ===== private methods =====

        _currentUIStateChanged: function () {
          switch (this._currentUIState) {
            case this._uiStateEnum.WAITING_FOR_DEVICE_ACCESS:
              this._onWaitingForDeviceAccess();
              break;
            case this._uiStateEnum.DEVICE_ACCESS_GRANTED:
              this._onDeviceAccessGranted();
              break;
            default:
              break;
          }
        },

        _onDeviceAccessGranted: function () {
          // we must short delay to catch the DOM changes. This is a common Polymer pattern.
          this.async(function () {
            // init live videostream
            if (this.mode == 'image' || this.mode == 'video') {
              var liveVideo = this._id('#liveVideo');
              liveVideo.src = window.URL.createObjectURL(this._stream);
              liveVideo.play();
            }

            // init image component
            if (this.mode == 'image') {
              this._imageComponent = new IvxCoreImage();
              this._imageComponent.init(this._id('#liveVideo'), this._videoDim.width, this._videoDim.height);
            }

            // init audio component
            if (this.mode == 'audio' || this.mode == 'video') {
              this._audioComponent = this._id('#audioComponent');
              this._audioComponent.init(this._stream);
            }

            // init video component
            if (this.mode == 'video') {
              this._videoComponent = new IvxCoreVideo();
              this._videoComponent.init(this._id('#liveVideo'), this._videoDim.width, this._videoDim.height);
            }
          });
        },

        _onWaitingForDeviceAccess: function () {
          // we must short delay to catch the DOM changes. This is a common Polymer pattern.
          this.async(function () {
            // get device sources
            try {
              var mst = new IvxCoreMediaStreamTrack();
              mst.getDevices(this._onDevicesReady.bind(this));
            } catch (e) {
              console.error(e);
            }
          });
        },

        _onDevicesReady: function(devices) {
          this._microphones = devices.microphones;
          this._cameras = devices.cameras;

          // default is first? *assumption*
          this._selectedCameraId = this._cameras[0].id;
          this._selectedMicrophoneId = this._microphones[0].id;
        },

        // Resets the UI State to the second state available (after the user granted access to the mic + webcam).
        // This method is used to do the first initializing (BEFORE the very first recording) and to reset the
        // state AFTER a recording was made.
        _initVideoAudio: function () {
          this._currentUIState = this._uiStateEnum.DEVICE_ACCESS_GRANTED;
        },

        // Initializes the MediaCapture API
        _initGetUserMedia: function () {
          var usermediaConfig = {
            enabled: {
              audio: this.mode == 'image' ? false : true,
              video: this.mode == 'audio' ? false : true
            },
            audio: {
              mandatory: {
                googEchoCancellation: false,
                googAutoGainControl: false,
                googNoiseSuppression: false,
                googHighpassFilter: false
              },
              optional: [{sourceId: this._selectedMicrophoneId}]
            },
            video: {
              mandatory: {
                maxWidth: this._videoDim.width,
                maxHeight: this._videoDim.height
              },
              optional: [{sourceId: this._selectedCameraId}]
            }
          };

          var umc = new IvxCoreUserMedia();
          umc.init(usermediaConfig, this._onUserMediaSuccess.bind(this), this._onUserMediaError.bind(this));
        },

        _onUserMediaSuccess: function (stream) {
          // save the stream in this object to access it everywhere
          this._stream = stream;
          this._initVideoAudio();
        },

        _onUserMediaError: function (error) {
          switch (error.name) {
            case 'PermissionDeniedError':
              alert('It seems that you have not allowed your Browser to access any media device. Please remove that ' +
              'restriction and try again.');
              break;
          }
        },

        // retrieves the data url from the image component
        _takePicture: function () {
          this._imageDataURL = this._imageComponent.getPictureDataURL('image/jpeg', 0.92);
        },

        // updates the length of the current recording every second
        _updateRecordingDuration: function () {
          var audioComponent = this._audioComponent;
          var update = function () {
            // @todo as long as this component is just a prototype we ignore the fact, that audio and video is not
            // correctly synced. We just play them both at the same time. The audio component ist the component wich
            // is used in video and audio mode, so we just use it to get the duration of the recording.
            this._recordingTime = Math.round(audioComponent.getDuration() / 1000);

            if (this._recordingNow == true) {
              this._updateRecordingDuration();
            }
          };

          update();
          this.async(update, null, 1000);
        },

        // starts the record for the current mode
        _startRecord: function () {
          if (this.mode == 'video') {
            this._videoComponent.startRecord();
          }
          this._audioComponent.startRecord();
          this._updateRecordingDuration();
        },

        // stops the record for the current mode
        _stopRecord: function () {
          if (this.mode == 'video' || this.mode == 'audio') {
            this._totalDuration = (this._audioComponent.getDuration() / 1000);
          }

          // switch from live to recorded video
          this._currentUIState = this._uiStateEnum.MEDIA_RECORDED;
          this._currentPlayerState = this._playerStateEnum.STOPPED;

          // video
          if (this.mode == 'video') {
            this._videoComponent.stopRecord();
            this._videoComponent.getRecordedBlob((function(videoInfo) {
              this._videoBlob = videoInfo.blob;
              this._videoFramesCaptured = videoInfo.frames;
              this._videoFps = videoInfo.fps;

              var playbackVideo = this._id('#playbackVideo');
              this._id('#playbackVideo').src = window.URL.createObjectURL(this._videoBlob);  // crossbrowser safe!
              playbackVideo.style.width = this._videoDim.width * 2 + 'px';
              playbackVideo.style.height = this._videoDim.height * 2 + 'px';
            }).bind(this));
          }

          // audio
          this._audioComponent.stopRecord();
          this._audioComponent.getRecordedBlob((function (audioBlob) {
            this._audioBlob = audioBlob;
            this._audioObjectURL = window.URL.createObjectURL(this._audioBlob);
          }).bind(this));
        }
      }, mixinsHelper));
    })();
  </script>
</polymer-element>
