<!--
`ivx-qtype-usermedia` can be used to record audio and video from the devices microphone and camera. It uses only the
standard APIs **Web Audio API** and **UserMedia API**. It also overlays the live video output with a audio visualization.

Please note that this component is a fancy wrapper around those APIs and provides you with the essential functionality
for recording video and audio. There are also some filters added, which the user can apply on the video. These filters
are merged into the video stream. They're only applied via the CSS3 selector `filter`.
The APIs are currently (January 2015) only available in the Blink engine. Trident and Gecko doesn't provide those API
and because there's no fallback mechanism implemented in this component, you should implement those by yourself or
use another implementation to fix this.

Please note too that the audio and video stream can not be recorded as one blob record. This is because of the API
limitation. As a result of that a video exists of a recorded video stream and a recorded audio stream. The recorded
audio and video will be started at the same time, when the user plays a recorded video. There could be some asynchronity
by playing the video

Use the attribute `mode` to switch between the types of recording (video (audio+video), audio (only audio), image).

== **Please Note** ==

This component only work on desktop browsers reliable:

- iOS 8+ on Safari only supports the Audio WebAPI, so there's no Video/Image access available
- Android 4.4.4+ on Chrome 40+ supports Audio and Video, 5+ integrates the evergreen Chrome, so it works there too
- Windows Internet Explorer does NOT support any of the used APIs

== **Please Note** ==

You can use the `ivx-qtype-usermedia` element like the following.

Example:

    <ivx-qtype-usermedia mode="video"></ivx-qtype-usermedia>

@group Inovex Survey Elements
@element ivx-qtype-usermedia
@homepage inovex.de
-->

<link rel="import" href="../../bower_components/polymer/polymer.html">
<link rel="import" href="../../bower_components/core-icons/av-icons.html">
<link rel="import" href="../../bower_components/core-icons/image-icons.html">
<link rel="import" href="../ivx-core-audio/ivx-core-audio.html">

<polymer-element name="ivx-qtype-usermedia" attributes="mode visualizeAudio">
  <template>
    <!-- Media Encoder WebM -->
    <script src="../../bower_components/whammy/whammy.js"></script>

    <!-- Stylesheet -->
    <link rel="stylesheet" href="ivx-qtype-usermedia.css">

    <div vertical layout center>

      <!-- START video / device container -->
      <div horizontal layout center>

        <!-- *** video container *** -->
        <div id="videoContainer" class="[[ mode ]]" flex>
          <template if="{{ _currentUIState == 'deviceAccessGranted' }}">
            <template if="[[ mode == 'video' || mode == 'image' ]]">
              <!-- live camera -->
              <video flex id="liveVideo" class="{{ _filterSelected }}" muted></video>
            </template>

            <!-- visualizer -->
            <template if="[[ mode == 'audio' || mode == 'video' ]]">
              <ivx-core-audio
                id="audioComponent"
                visualizeAudio="{{ visualizeAudio }}"
                gain="{{ _gainValue }}"
                >
              </ivx-core-audio>
            </template>
          </template>

          <template if="{{ _currentUIState == 'mediaRecorded' && (mode == 'video' || mode == 'audio') }}">
            <!-- playback from existing video -->
            <template if="[[ mode == 'video' ]]">
              <video id="playbackVideo" style="width:{{ _videoCanvas.width }}px; height: {{ _videoCanvas.height }}px;" class="{{ _filterSelected }}" on-ended="{{ onPlaybackEnded }}" on-timeupdate="{{ onPlaybackTimeupdate }}"></video>
              <audio id="playbackAudio" src="{{ _audioObjectURL }}" volume="1.0" type="audio/wave"></audio>
            </template>

            <template if="[[ mode == 'audio' ]]">
              <audio id="playbackAudio" src="{{ _audioObjectURL }}" volume="1.0" type="audio/wave" on-ended="{{ onPlaybackEnded }}" on-timeupdate="{{ onPlaybackTimeupdate }}"></audio>
            </template>

            <!-- progress line -->
            <div id="progressLine"></div>
          </template>

          <template if="{{ _currentUIState == 'mediaRecorded' && mode == 'image' }}">
            <!-- picture from camera -->
            <img src="{{ _imageDataURL }}" class="{{ _filterSelected }}">
          </template>
        </div>

        <!-- *** device list *** -->
        <div self-end> <!-- positioning on bottom -->
          <div id="deviceList" layout vertical>
            <template if="{{ (mode == 'video' || mode == 'image') && _devices.cameras.length > 0 }}">
              <paper-radio-group selected="{{ _devices.selected.cameraId }}" class="spaceToTop" layout vertical>
                <template repeat="{{ device, index in _devices.cameras }}">
                  <paper-radio-button
                    name="{{ device.id }}"
                    label="Kamera {{ index + 1 }} {{ device.facing != '' ? '(' + device.facing + ')' : '' }}"
                    on-tap="{{ onCameraTapped }}">
                  </paper-radio-button>
                </template>
              </paper-radio-group>
            </template>

            <template if="[[ mode == 'video' || mode == 'audio' ]]">
              <paper-radio-group selected="{{ _selectedMicrophoneId }}" class="spaceToTop" layout vertical>
                <template repeat="{{ device, index in _devices.microphones }}">
                  <paper-radio-button
                    name="{{ device.id }}"
                    label="Mikrofon {{ index + 1 }} {{ device.facing != '' ? '(' + device.facing + ')' : '' }}"
                    on-tap="{{ onMicrophoneTapped }}">
                  </paper-radio-button>
                </template>
              </paper-radio-group>

              <paper-slider min="0" max="100" value="[[ _microphoneGainLevelInit * 100 ]]" on-immediate-value-change="{{ onMicrophoneVolumeChange }}"></paper-slider>
            </template>
          </div>
        </div>
      </div>
      <!-- END video / device container -->

      <!-- *** button container *** -->
      <div id="buttonContainer" class="spaceToTop">
        <!-- First enable camera/mic to record anything -->
        <template if="{{ _currentUIState == 'waitingForDeviceAccess' }}">

          <!-- activate button -->
          <paper-button raised on-tap="{{ onEnableTapped }}" class="spaceToTop">
            <core-icon icon="av:videocam"></core-icon>
            <div>Zugriff aktivieren</div>
          </paper-button>

        </template>

        <template if="{{ _currentUIState == 'deviceAccessGranted' }}">
          <div center layout horizontal>
            <div center layout vertical>

              <template bind ref="audioVideoImageButtons"></template>

              <!-- filters -->
              <template if="[[ mode == 'video' || mode == 'image' ]]">
                <paper-radio-group selected="{{ _filterSelected }}" class="spaceToTop">
                  <template repeat="{{ filter, index in _filters }}">
                    <paper-radio-button
                      name="{{ filter }}"
                      label="{{ filter }}"
                      on-tap="{{ onFilterTapped }}">
                    </paper-radio-button>
                  </template>
                </paper-radio-group>
              </template>
            </div>
          </div>
        </template>

        <template if="{{ _currentUIState == 'mediaRecorded' }}">
          <div center layout horizontal>

            <template if="[[ mode == 'audio' || mode == 'video' ]]">
              <!-- play button -->
              <template if="{{ _playerMode == 'mediaStopped' || _playerMode == 'mediaPaused' }}">
                <paper-button id="playButton" raised on-tap="{{ onPlayTapped }}">
                  <core-icon icon="av:play-arrow"></core-icon>
                </paper-button>
              </template>

              <!-- pause button -->
              <template if="{{ _playerMode == 'mediaPlaying' }}">
                <paper-button id="pauseButton" raised on-tap="{{ onPauseTapped }}">
                  <core-icon icon="av:pause"></core-icon>
                </paper-button>
              </template>

              <!-- stop button -->
              <paper-button id="stopButton" raised on-tap="{{ onStopTapped }}">
                <core-icon icon="av:stop"></core-icon>
              </paper-button>
            </template>

            <template if="[[ mode == 'audio' || mode == 'video' || mode == 'image' ]]">
              <!-- redo button -->
              <paper-button raised on-tap="{{ onRedoTapped }}">
                <core-icon icon="av:replay"></core-icon>
                <div>Aufnahme wiederholen</div>
              </paper-button>
            </template>
          </div>
        </template>
      </div>

      <div id="consoleContainer" class="spaceToTop">
        <template if="{{ (mode == 'audio' || mode == 'video') && _recordingNow }}">
          Recording ... {{ _recordingTime }}s
        </template>

        <template if="{{ mode == 'audio' && !_recordingNow && _currentUIState == 'mediaRecorded' }}">
          Recorded {{ _totalDuration }}s
        </template>

        <template if="{{ mode == 'video' && !_recordingNow && _currentUIState == 'mediaRecorded' }}">
          Frames captured: {{ _videoFrames.length }}, {{ _totalDuration }}s Video (~{{ _fps }} fps)
        </template>
      </div>
    </div>

    <!-- START bound templates -->
    <template id="audioVideoImageButtons">
      <template if="[[ mode == 'audio' || mode == 'video' ]]">
        <paper-button id="recordButton" raised on-tap="{{ onRecordingTapped }}">

          <!-- glow/pulsate the record icon in recording mode -->
          <template if="{{ !_recordingNow }}">
            <core-icon icon="radio-button-off"></core-icon>
          </template>

          <template if="{{ _recordingNow }}">
            <core-icon icon="radio-button-on" class="pulsate"></core-icon>
          </template>

          <div>
            <template if="{{ !_recordingNow }}">Aufnahme starten</template>
            <template if="{{ _recordingNow }}">Aufnahme stoppen</template>
          </div>
        </paper-button>
      </template>

      <template if="[[ mode == 'image' ]]">
        <paper-button id="snapButton" raised on-tap="{{ onSnapTapped }}">
          <core-icon icon="image:photo-camera"></core-icon>
          <div>Bild aufnehmen</div>
        </paper-button>
      </template>
    </template>
    <!-- END bound templates -->

  </template>

  <script>
    (function () {
      Polymer({
        /**
         * The usermedia mode to record from. Valid modes are `video`, `audio` and `image`.
         *
         * @attribute mode
         * @type String
         * @default 'video'
         */

        /**
         * Enables or disables the audio visualizer.
         *
         * @attribute visualizeAudio
         * @type Boolean
         * @default true
         */
        publish: {
          mode: 'video',
          visualizeAudio: true
        },

        // internal resource for storing the width and height of space available
        _viewport: {
          width: 0,
          height: 0
        },

        // filters to apply on the video via CSS3
        _filters: ['Kein Filter', 'grayscale', 'sepia', 'invert', 'brightness', 'blur'],
        _filterSelected: 'Kein Filter',

        // the UI states
        _uiStateEnum: {
          WAITING_FOR_DEVICE_ACCESS: 'waitingForDeviceAccess',
          DEVICE_ACCESS_GRANTED: 'deviceAccessGranted',
          MEDIA_RECORDED: 'mediaRecorded'
        },
        _currentUIState: '',
        _playerMode: 'mediaStopped',  // substate
        _recordingNow: false,

        // image specific
        _imageDataURL: null,

        // video specific
        _videoDim: {
          width: 320,
          height: 240
        },
        _videoCanvas: document.createElement('canvas'), // offscreen canvas
        _videoFrames: [],
        _fps: 0,
        // stores the video recorded from the stream as blob
        _videoBlob: null,

        _microphoneGainLevelInit: 0.8,
        _gainValue: 0.8,
        _volumeFullNode: null,

        // the stream object (getUserMedia)
        _stream: null,

        // startTime for the recording
        _startTime: null,
        // endTime for the recording
        _endTime: null,
        // duration for the recording
        _totalDuration: null,
        // will run up when in recording mode
        _recordingTime: 0,

        // requestAnimationFrame ID
        _rafId: null,

        // device sources
        _devices: {
          microphones: [],
          cameras: [],
          other: [],
          selected: {
            cameraId: '',
            microphoneId: ''
          }
        },

        // called when usermedia is ready
        ready: function() {
          // setting the current UI state to waiting for access (earliest state)
          this._currentUIState = this._uiStateEnum.WAITING_FOR_DEVICE_ACCESS;

          this._compatibilityCheck();
          this._detectDeviceSources();

          if (this.mode == 'video' || this.mode == 'image') {
            // canvas is only the half of the size
            this._videoCanvas.width = this._videoDim.width * 2;
            this._videoCanvas.height = this._videoDim.height * 2;
          }
        },

        _compatibilityCheck: function() {
          if (!(window.AudioContext || window.webkitAudioContext)) {
            console.log('AudioContext is not supported => No Audio functionality available :(');
          }

          if (!(navigator.getUserMedia ||
                navigator.webkitGetUserMedia ||
                navigator.mozGetUserMedia ||
                navigator.msGetUserMedia)) {
            console.log('getUserMedia is not supported => No Video functionality available :(');
          }
        },

        // detects the device sources available to the browser
        _detectDeviceSources: function() {
          if (typeof MediaStreamTrack === 'undefined') {
            console.log('Your browser does NOT support MediaStreamTrack, no device detection possible :(');
            return;
          }

          var sortDevices = function(sources) {
            this._devices.microphones = [];
            this._devices.cameras = [];

            for (var i = 0; i < sources.length; i++) {
              if (sources[i].kind == 'audio') {
                this._devices.microphones.push(sources[i]);
              } else if (sources[i].kind == 'video') {
                this._devices.cameras.push(sources[i]);
              } else {
                this._devices.other.push(sources[i]);
              }
            }

            var me = this;
            var checkCameras = function() {
              if (me._devices.cameras.length == 0) {
                console.log('You need at least one camera to use that component - could not find any :(');
                return false;
              }
            };

            var checkMicrophones = function() {
              if (me._devices.microphones.length == 0) {
                console.log('You need at least one microphone to use that component - could not find any :(');
                return false;
              }
            };

            if (this.mode == 'audio') {
              if (checkMicrophones() === false) {
                return;
              }
            }

            if (this.mode == 'video') {
              if (checkMicrophones() === false || checkCameras() === false) {
                return;
              }
            }

            if (this.mode == 'image') {
              if (checkCameras() === false) {
                return;
              }
            }

            // default is first? *assumption*
            this._devices.selected.cameraId = this._devices.cameras[0].id;
            this._selectedMicrophoneId = this._devices.microphones[0].id;
          };

          MediaStreamTrack.getSources(sortDevices.bind(this)); // not implemented under Android 4.4.4 :(
        },

        _currentUIStateChanged: function() {
          if (this._currentUIState == this._uiStateEnum.DEVICE_ACCESS_GRANTED) {
            // we must short delay to catch the DOM changes. This is a common Polymer pattern,
            // see https://www.polymer-project.org/docs/polymer/polymer.html#asyncmethod for more infos.
            this.async(function () {
              // init video
              if (this.mode != 'audio') {
                this._initVideo();
              }

              if (this.mode != 'image') {
                // init audio
                this._initAudioContext();
              }
            });
          }
        },

        // short helper method to query the shadowRoot DOM
        _shortSelect: function(selector) {
          return this.shadowRoot.querySelector(selector);
        },

        _takePicture: function() {
          this._resetCanvas();

          this._videoCanvas.getContext('2d').drawImage(
            this._shortSelect('#liveVideo'),
            0,
            0,
            this._videoCanvas.width,
            this._videoCanvas.height
          );

          // Read back canvas as jpeg.
          this._imageDataURL = this._videoCanvas.toDataURL('image/jpeg', 0.92);
        },

        _updateRecordingDuration: function() {
          var update = function() {
            this._recordingTime = Math.round((Date.now() - this._startTime) / 1000);

            if (this._recordingNow == true) {
              this._updateRecordingDuration();
            }
          };

          update();
          this.async(update, null, 1000);
        },

        // starts the record for the current mode
        _startRecord: function() {
          this._startTime = Date.now();

          if (this.mode == 'video') {
            this._startRecordVideo();
          }
          this._startRecordAudio();

          this._updateRecordingDuration();
        },

        // resets the canvas context
        _resetCanvas: function() {
          this._videoCanvas.getContext('2d').clearRect(0, 0, this._videoCanvas.width, this._videoCanvas.height);
        },

        // starts the record of the video stream
        _startRecordVideo: function() {
          this._videoBlob = null; // reset blob
          this._videoFrames = [];
          this._resetCanvas();

          // draw the current video frame into the canvas 2D context to read them back as image.
          function drawVideoFrame() {
            this._rafId = requestAnimationFrame((drawVideoFrame).bind(this)); // cross browser safe!
            this._videoCanvas.getContext('2d').drawImage(this._shortSelect('#liveVideo'), 0, 0, this._videoCanvas.width, this._videoCanvas.height);

            // Read back canvas as webp.
            var url = this._videoCanvas.toDataURL('image/webp', 0.80); // image/jpeg is a way faster :(
            this._videoFrames.push(url);
          };

          // request the first frame
          this._rafId = requestAnimationFrame((drawVideoFrame).bind(this));
        },

        // starts the audio recording
        _startRecordAudio: function() {
          this._shortSelect('#audioComponent').startRecord();
        },

        // stops the record for the current mode
        _stopRecord: function() {
          this._endTime = Date.now();
          this._totalDuration = ((this._endTime - this._startTime) / 1000);

          // switch from live to recorded video
          this._currentUIState = this._uiStateEnum.MEDIA_RECORDED;
          this._playerMode = 'mediaStopped';

          if (this.mode == 'video') {
            this._stopRecordVideo();
            this._embedVideoPreview();
          }

          this._stopRecordAudio();
          this._embedAudioPreview();
        },

        // stops the record for the video stream
        _stopRecordVideo: function() {
          cancelAnimationFrame(this._rafId);
          // adjust the recorded frames to the duration, so we get the real speed!
          this._fps = Math.floor(this._videoFrames.length / this._totalDuration);
          this._videoBlob = Whammy.fromImageArray(this._videoFrames, this._fps);
        },

        // stops the record of the audio stream
        _stopRecordAudio: function() {
          this._shortSelect('#audioComponent').stopRecord();
/*
          this._audioRecorder.stop();
*/
        },

        _embedAudioPreview: function() {
          this._shortSelect('#audioComponent').getRecordedBlob((function(audioBlob) {
            this._audioObjectURL = window.URL.createObjectURL(audioBlob);
          }).bind(this));
        },

        // sets the video config
        _embedVideoPreview: function() {
          // adjust size of player to video size
          this.job('waitForDOMChanges', function() {
            var playbackVideo = this._shortSelect('#playbackVideo');

            playbackVideo.style.width = this._videoCanvas.width + 'px';
            playbackVideo.style.height = this._videoCanvas.height + 'px';

            // add blob to src of video element
            playbackVideo.src = window.URL.createObjectURL(this._videoBlob);  // crossbrowser safe!
          });
        },

        // attached callback, called when the template elements are attached to the DOM
        attached: function() {
          this.job('delayAbit', function() {
            this._viewport.width = this.offsetWidth;
            this._viewport.height = this.offsetHeight;
          });
        },

        // event handler to reinit the UserMedia stream, when the camera source changes
        onCameraTapped: function(event, detail, sender) {
          if (this._currentUIState == this._uiStateEnum.DEVICE_ACCESS_GRANTED && !this._recordingNow) {
            this._initGetUserMedia();
          }
        },

        // event handler to reinit the UserMedia stream, when the microphone source changes
        onMicrophoneTapped: function(event, detail, sender) {
          if (this._currentUIState == this._uiStateEnum.DEVICE_ACCESS_GRANTED && !this._recordingNow) {
            this._initGetUserMedia();
          }
        },

        onMicrophoneVolumeChange: function(event, detail, sender) {
          if (this._currentUIState == this._uiStateEnum.DEVICE_ACCESS_GRANTED) {
//            this._volumeFullNode.gain.value = sender.immediateValue / 100;
            this._gainValue = sender.immediateValue / 100;
          }
        },

        // event handler to initialize the UserMedia API
        onEnableTapped: function(event, detail, sender) {
          this._initGetUserMedia();
        },

        // event handler to grap a picture of the current stream frame
        onSnapTapped: function(event, detail, sender) {
          this._takePicture();
          this._currentUIState = this._uiStateEnum.MEDIA_RECORDED;
        },

        // event handler to start and stop recording
        onRecordingTapped: function(event, detail, sender) {
          this._recordingNow = !this._recordingNow;

          // recording
          if (this._recordingNow) {
            this._startRecord();
          } else {
            this._stopRecord();
          }
        },

        // event handler to apply the selected filter
        onFilterTapped: function(event, detail, sender) {
          this._filterSelected = sender.getAttribute('name');
        },

        // event handler to hack the width of the
        onPlaybackEnded: function(event, detail, sender) {
          this._playerMode = 'mediaStopped';
          this._shortSelect('#progressLine').style.width = '100%';
        },

        // event handler to sync the current playback time with the visual progress bar
        onPlaybackTimeupdate: function(event, detail, sender) {
          var percentPlayed = (sender.currentTime / this._totalDuration) * 100;
          this._shortSelect('#progressLine').style.width = percentPlayed + '%';
        },

        // event handler to start playing the recorded stream(s) (based on the current mode)
        onPlayTapped: function(event, detail, sender) {
          this._playerMode = 'mediaPlaying';

          if (this.mode == 'video') {
            this._shortSelect('#playbackVideo').play();
          }
          this._shortSelect('#playbackAudio').play();
        },

        // event handler to pause the playing audio/video (based on the current mode)
        onPauseTapped: function(event, detail, sender) {
          this._playerMode = 'mediaPaused';

          if (this.mode == 'video') {
            this._shortSelect('#playbackVideo').pause();
          }
          this._shortSelect('#playbackAudio').pause();
        },

        // event handler to stop the playing audio/video (based on the current mode)
        onStopTapped: function(event, detail, sender) {
          this._playerMode = 'mediaStopped';

          if (this.mode == 'video') {
            this._shortSelect('#playbackVideo').load();
          }
          this._shortSelect('#playbackAudio').load();
        },

        // event handler to dismiss the current recording to start a new one
        onRedoTapped: function(event, detail, sender) {
          this._initVideoAudio();
        },

        _initVideo: function() {
          var liveVideo = this._shortSelect('#liveVideo');
          liveVideo.src = window.URL.createObjectURL(this._stream);
          liveVideo.play();
        },

        // initializes the audio context. See source code below for more comments and details.
        _initAudioContext: function() {
          this._shortSelect('#audioComponent').init(this._stream);
        },

        // Resets the UI State to the second state available (after the user granted access to the mic + webcam).
        // This method is used to do the first initializing (BEFORE the very first recording) and to reset the
        // state AFTER a recording was made.
        _initVideoAudio: function() {
          this._currentUIState = this._uiStateEnum.DEVICE_ACCESS_GRANTED;
        },

        // Initializes the MediaCapture API
        _initGetUserMedia: function() {
          navigator.getUserMedia = (
            navigator.getUserMedia ||
            navigator.webkitGetUserMedia ||
            navigator.mozGetUserMedia ||
            navigator.msGetUserMedia
          );

          var videoConfig = {
            mandatory: {
              maxWidth: this._videoDim.width,
              maxHeight: this._videoDim.height
            },
            optional: [{ sourceId: this._devices.selected.cameraId }]
          };

          var audioConfig = {
            mandatory: {
              googEchoCancellation: false,
              googAutoGainControl: false,
              googNoiseSuppression: false,
              googHighpassFilter: false
            },
            optional: [{ sourceId: this._selectedMicrophoneId }]
          };

          // if the mode is image, we don't need any audio
          if (this.mode == 'image') {
            audioConfig = false;
          }

          // if the mode is audio, we don't need any video
          if (this.mode == 'audio') {
            videoConfig = false;
          }

          if (navigator.getUserMedia) {
            // see https://developer.mozilla.org/en-US/docs/NavigatorUserMedia.getUserMedia
            navigator.getUserMedia (

              // constraints
              {
                video: videoConfig,
                audio: audioConfig
              },

              // successCallback
              (function(stream) {
                // save the stream in this object to access it everywhere
                this._stream = stream;
                this._initVideoAudio();
              }).bind(this),

              // errorCallback
              function(err) {
                switch(err.name) {
                  case 'PermissionDeniedError':
                    alert('It seems that you have not allowed your Browser to access any media device. Please remove that ' +
                    'restriction and try again.');
                    break;
                }
              }
            );
          } else {
            console.log('getUserMedia API is NOT supported in your browser :(');
          }
        }
      });
    })();
  </script>
</polymer-element>
